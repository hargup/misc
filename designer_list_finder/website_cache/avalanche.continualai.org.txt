





Avalanche: an End-to-End Library for Continual Learning - Avalanche











AvalancheAvalanche - v0.4.0GitHubAPI DocCL-BaselinesAvalanche-RLSearchâŒƒKLinksAvalanche: an End-to-End Library for Continual LearningğŸ“ŒGetting StartedIntroductionCurrent ReleaseHow to InstallLearn Avalanche in 5 MinutesğŸ“™From Zero to Hero TutorialIntroductionModelsBenchmarksTrainingEvaluationLoggersPutting All TogetherExtending AvalancheContribute to AvalancheHow-TosAvalancheDatasetDataloaders, Buffers, and ReplaySave and load checkpointsğŸ“ExamplesModelsBenchmarksTrainingEvaluationLoggersğŸ’»Code DocumentationAvalanche APIHow to ContributeGuidelinesâ“Questions and IssuesAsk Your QuestionAdd Your IssueRequest a FeatureGive FeedbackFAQğŸ‘ªAbout UsThe PeopleJoin Us!SlackEmailTwitterPowered By GitBookAvalanche: an End-to-End Library for Continual LearningPowered by ContinualAIAvalanche is an End-to-End Continual Learning Library based on PyTorch, born within ContinualAI with the goal of providing a shared and collaborative open-source (MIT licensed) codebase for fast prototyping, training and reproducible evaluation of continual learning algorithms.Looking for continual learning baselines? In the CL-Baseline sibling project based on Avalanche we reproduce seminal papers results you can directly use in your experiments!Avalanche can help Continual Learning researchers and practitioners in several ways:Write less code, prototype faster & reduce errorsImprove reproducibility, modularity and reusabilityIncrease code efficiency, scalability & portabilityAugment impact and usability of your research productsThe library is organized in five main modules:Benchmarks: This module maintains a uniform API for data handling: mostly generating a stream of data from one or more datasets. It contains all the major CL benchmarks (similar to what has been done for torchvision).Training: This module provides all the necessary utilities concerning model training. This includes simple and efficient ways of implement new continual learning strategies as well as a set pre-implemented CL baselines and state-of-the-art algorithms you will be able to use for comparison!Evaluation: This modules provides all the utilities and metrics that can help evaluate a CL algorithm with respect to all the factors we believe to be important for a continually learning system.Models: In this module you'll be able to find several model architectures and pre-trained models that can be used for your continual learning experiment (similar to what has been done in torchvision.models).Logging: It includes advanced logging and plotting features, including native stdout, file and TensorBoard support (How cool it is to have a complete, interactive dashboard, tracking your experiment metrics in real-time with a single line of code?)Avalanche the first experiment of a End-to-end Library for reproducible continual learning research & development where you can find benchmarks, algorithms, evaluation metrics and much more, in the same place.Let's make it together ğŸ‘« a wonderful ride! ğŸˆCheck out how your code changes when you start using Avalanche! ğŸ‘‡With AvalancheWithout Avalancheimport torchfrom torch.nn import CrossEntropyLossfrom torch.optim import SGDâ€‹from avalanche.benchmarks.classic import PermutedMNISTfrom avalanche.training.plugins import EvaluationPluginfrom avalanche.evaluation.metrics import accuracy_metricsfrom avalanche.models import SimpleMLPfrom avalanche.training.supervised import Naiveâ€‹# Configdevice = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")â€‹# modelmodel = SimpleMLP(num_classes=10)â€‹# CL Benchmark Creationperm_mnist = PermutedMNIST(n_experiences=3)train_stream = perm_mnist.train_streamtest_stream = perm_mnist.test_streamâ€‹# Prepare for training & testingoptimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)criterion = CrossEntropyLoss()eval_plugin = EvaluationPlugin(    accuracy_metrics(minibatch=True, epoch=True, epoch_running=True,                      experience=True, stream=True))â€‹# Continual learning strategycl_strategy = Naive(    model, optimizer, criterion, train_mb_size=32, train_epochs=2,     eval_mb_size=32, evaluator=eval_plugin, device=device)â€‹# train and test loopresults = []for train_task in train_stream:    cl_strategy.train(train_task, num_workers=4)    results.append(cl_strategy.eval(test_stream))import torchimport torch.nn as nnfrom torch.nn import CrossEntropyLossfrom torch.optim import SGDfrom torchvision import transformsfrom torchvision.datasets import MNISTfrom torchvision.transforms import ToTensor, RandomCropfrom torch.utils.data import DataLoaderimport numpy as npfrom copy import copyâ€‹# Configdevice = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")â€‹# modelclass SimpleMLP(nn.Module):â€‹ def __init__(self, num_classes=10, input_size=28*28): super(SimpleMLP, self).__init__()â€‹        self.features = nn.Sequential(            nn.Linear(input_size, 512),            nn.ReLU(inplace=True),            nn.Dropout(), )        self.classifier = nn.Linear(512, num_classes)        self._input_size = input_sizeâ€‹ def forward(self, x):        x = x.contiguous()        x = x.view(x.size(0), self._input_size)        x = self.features(x)        x = self.classifier(x) return xmodel = SimpleMLP(num_classes=10)â€‹# CL Benchmark Creationlist_train_dataset = []list_test_dataset = []rng_permute = np.random.RandomState(0)train_transform = transforms.Compose([    RandomCrop(28, padding=4),    ToTensor(),    transforms.Normalize((0.1307,), (0.3081,))])test_transform = transforms.Compose([    ToTensor(),    transforms.Normalize((0.1307,), (0.3081,))])â€‹# permutation transformationclass PixelsPermutation(object): def __init__(self, index_permutation):        self.permutation = index_permutationâ€‹ def __call__(self, x): return x.view(-1)[self.permutation].view(1, 28, 28)â€‹def get_permutation(): return torch.from_numpy(rng_permute.permutation(784)).type(torch.int64)â€‹# for every incremental steppermutations = []for i in range(3): # choose a random permutation of the pixels in the image    idx_permute = get_permutation()    current_perm = PixelsPermutation(idx_permute)    permutations.append(idx_permute)â€‹ # add the permutation to the default dataset transformation    train_transform_list = train_transform.transforms.copy()    train_transform_list.append(current_perm)    new_train_transform = transforms.Compose(train_transform_list)â€‹    test_transform_list = test_transform.transforms.copy()    test_transform_list.append(current_perm)    new_test_transform = transforms.Compose(test_transform_list)â€‹ # get the datasets with the constructed transformation    permuted_train = MNIST(root='./data/mnist',                           download=True, transform=new_train_transform)    permuted_test = MNIST(root='./data/mnist',                    train=False,                    download=True, transform=new_test_transform)    list_train_dataset.append(permuted_train)    list_test_dataset.append(permuted_test)â€‹# Trainoptimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)criterion = CrossEntropyLoss()â€‹for task_id, train_dataset in enumerate(list_train_dataset):â€‹    train_data_loader = DataLoader(        train_dataset, num_workers=4, batch_size=32)  for ep in range(2): for iteration, (train_mb_x, train_mb_y) in enumerate(train_data_loader):            optimizer.zero_grad()            train_mb_x = train_mb_x.to(device)            train_mb_y = train_mb_y.to(device)â€‹ # Forward            logits = model(train_mb_x) # Loss            loss = criterion(logits, train_mb_y) # Backward            loss.backward() # Update            optimizer.step()â€‹# Testacc_results = []for task_id, test_dataset in enumerate(list_test_dataset):     test_data_loader = DataLoader(        test_dataset, num_workers=4, batch_size=32)     correct = 0 for iteration, (test_mb_x, test_mb_y) in enumerate(test_data_loader):â€‹ # Move mini-batch data to device        test_mb_x = test_mb_x.to(device)        test_mb_y = test_mb_y.to(device)â€‹ # Forward        test_logits = model(test_mb_x)â€‹ # Loss        test_loss = criterion(test_logits, test_mb_y)â€‹ # compute acc        correct += test_mb_y.eq(test_logits.argmax(dim=1)).sum().item()     acc_results.append(correct / len(test_dataset))ğŸš¦ Getting StartedWe know that learning a new tool may be tough at first. This is why we made Avalanche as easy as possible to learn with a set of resources that will help you along the way.For example, you may start with our 5-minutes guide that will let you acquire the basics about Avalanche and how you can use it in your research project:Learn Avalanche in 5 MinutesWe have also prepared for you a large set of examples & snippets you can plug-in directly into your code and play with:ğŸ“ExamplesHaving completed these two sections, you will already feel with superpowers âš¡, this is why we have also created an in-depth tutorial that will cover all the aspect of Avalanche in details and make you a true Continual Learner! ğŸ‘¨â€ğŸ“ï¸ğŸ“™From Zero to Hero TutorialğŸ“‘ Cite AvalancheIf you used Avalanche in your research project, please remember to cite our reference paper "Avalanche: an End-to-End Library for Continual Learning". This will help us make Avalanche better known in the machine learning community, ultimately making a better tool for everyone:@InProceedings{lomonaco2021avalanche,    title={Avalanche: an End-to-End Library for Continual Learning},    author={Vincenzo Lomonaco and Lorenzo Pellegrini and Andrea Cossu and Antonio Carta and Gabriele Graffieti and Tyler L. Hayes and Matthias De Lange and Marc Masana and Jary Pomponi and Gido van de Ven and Martin Mundt and Qi She and Keiland Cooper and Jeremy Forest and Eden Belouadah and Simone Calderara and German I. Parisi and Fabio Cuzzolin and Andreas Tolias and Simone Scardapane and Luca Antiga and Subutai Amhad and Adrian Popescu and Christopher Kanan and Joost van de Weijer and Tinne Tuytelaars and Davide Bacciu and Davide Maltoni},    booktitle={Proceedings of IEEE Conference on Computer Vision and Pattern Recognition},    series={2nd Continual Learning in Computer Vision Workshop},    year={2021}}ğŸ—‚ï¸ Maintained by ContinualAI LabAvalanche is the flagship open-source collaborative project of ContinualAI: a non profit research organization and the largest open community on Continual Learning for AI.Do you have a question, do you want to report an issue or simply ask for a new feature? Check out the Questions & Issues center. Do you want to improve Avalanche yourself? Follow these simple rules on How to Contribute.The Avalanche project is maintained by the collaborative research team ContinualAI Lab and used extensively by the Units of the ContinualAI Research (CLAIR) consortium, a research network of the major continual learning stakeholders around the world.We are always looking for new awesome members willing to join the ContinualAI Lab, so check out our official website if you want to learn more about us and our activities, or contact us.Learn more about the Avalanche team and all the people who made it great!Next - Getting StartedIntroductionOn this pageğŸš¦ Getting StartedğŸ“‘ Cite AvalancheğŸ—‚ï¸ Maintained by ContinualAI Lab



