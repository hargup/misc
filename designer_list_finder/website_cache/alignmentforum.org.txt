


AI Alignment Forum



 This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. AI ALIGNMENT FORUMAFLoginHomeLibraryQuestionsAll PostsAboutHomeLibraryQuestionsAll PostsRecommended SequencesAGI safety from first principlesby Richard NgoEmbedded Agencyby Abram Demski2022 MIRI Alignment Discussionby Rob BensingerAI Alignment Posts50Welcome & FAQ!Ruben Bloom, Oliver Habryka2y85Apply to lead a project during the next virtual AI Safety CampLinda Linsefors, Remmelt Ellen15h048Explaining grokking through circuit efficiencyVikrant Varma, Rohin Shah6d63The Löbian Obstacle, And Why You Should Caremarc/er6d016Recreating the caring driveCatnee7d041ActAdd: Steering Language Models without Optimizationtechnicalities, Alex Turner, lisathiergart, David Udell, Ulisse Mini, Monte MacDiarmid7d283What I would do if I wasn’t at ARC EvalsLawrence Chan3h345Benchmarks for Detecting Measurement Tampering [Redwood Research]Ryan Greenblatt, Fabien Roger8d038Paper: On measuring situational awareness in LLMsOwain Evans, Daniel Kokotajlo, Mikita Balesni, Tomek Korbak, Lukas Berglund, Asa Cooper Stickland, Meg, Maximilian Kaufmann10d137Fundamental question: What determines a mind's effects?Tsvi Benson-Tilsen10d2Load MoreRecent DiscussionWhat I would do if I wasn’t at ARC Evals83Lawrence ChanCareersCommunityCurated8dIn which: I list 9 projects that I would work on if I wasn’t busy working on safety standards at ARC Evals, and explain why they might be good to work on. Epistemic status: I’m prioritizing getting this out fast as opposed to writing it carefully. I’ve thought for at least a few hours and talked to a few people I trust about each of the following projects, but I haven’t done that much digging into each of these, and it’s likely that I’m wrong about many material facts. I also make little claim to the novelty of the projects. I’d recommend looking into these yourself before committing to doing them. (Total time spent writing or editing this post: ~8 hours.)Standard disclaimer: I’m writing this in my own capacity. The views...(Continue Reading  – 3802 more words)Raymond Arnold3h11Curated. I liked both the concrete array of ideas coming from someone who has a fair amount of context, and the sort of background models I got from reading each of said ideas.Reply2Alana6dIs this no longer the case? If so, what changed?4Lawrence Chan6dI think this has gotten both worse and better in several ways.

It's gotten better in that ARC and Redwood (and to a lesser extent, Anthropic
and OpenAI) have put out significantly more of their research. FAR Labs also
exists is also doing some of the research proliferation that would've gone on
inside of Constellation. 

It's worse in that there's been some amount of deliberate effort to build more
of an AIS community in Constellation, e.g. with explicit Alignment Days where
people are encouraged to present work-in-progress and additional fellowships and
workshops. 

On net I think it's gotten better, mainly because there's just been a lot more
content put out in 2023 (per unit research) than in 2022. Apply to lead a project during the next virtual AI Safety Camp5Linda Linsefors, Remmelt EllenAI RiskAI Safety CampCommunityAIPersonal Blog15hThis is a linkpost for https://aisafety.camp/Do you have AI Safety research ideas that you would like others to work on? Is there a project you want to do and you want help finding a team to work with you? AI Safety Camp could be the solution for you!SummaryAI Safety Camp Virtual is a 3-month long online research program from January to April 2024, where participants form teams to work on pre-selected projects. We want you to suggest the projects!If you have an AI Safety project idea and some research experience, apply to be a Research Lead.If accepted, we offer some assistance to develop your idea into a plan suitable for AI Safety Camp. When project plans are ready, we open up team member applications. You get to review applications for your team,...(Continue Reading  – 1423 more words)Elements of Computational Philosophy, Vol. I: Truth6Paul Bricman, Tom FeeneyPhilosophyLanguage ModelsMetaethicsMachine Learning  (ML)Meta-PhilosophyWorld ModelingAIFrontpage2moThis is a linkpost for https://compphil.github.io/truth/We're excited to share the first volume of Elements of Computational Philosophy, an interdisciplinary and collaborative project series focused on operationalizing fundamental philosophical notions in ways that are natively compatible with the current paradigm in AI.The first volume paints a broad-strokes picture of operationalizing truth and truth-seeking. Beyond this high-level focus, its 100+ pages can be framed in several different ways, which is why we placed multiple topic-based summaries at the beginning of the document. The note to the reader and the table of contents should further help scope and navigate the document.Have a pleasant read, and feel free to use this linkpost to comment on the document as you go. Questions, criticism, and suggestions are all welcome.PS: There will soon be a presentation about the overarching project series as part of the alignment speaker series hosted by EleutherAI. Expect more information soon on the #announcements channel of their Discord server. In general, keep an eye on this space.Charlie Steiner2d10All I want for christmas is a "version for engineers." Here's how we constructed the reward, here's how we did the training, here's what happened over the course of training.My current impression is that the algorithm for deciding who wins an argument is clever, if computationally expensive, but you don't have a clever way to turn this into a supervisory signal, instead relying on brute force (which you don't have much of). I didn't see where you show that you managed to actually make the LLMs better arguers.Connection between winning an argument and finding the truth continues to seem plenty breakable both in humans and in AIs.ReplyTurnTrout's shortform feedAlex TurnerPersonal Blog4yAlex Turner2d9AI cognition doesn't have to use alien concepts to be uninterpretable. We've never fully interpreted human cognition, either, and we know that our introspectively accessible reasoning uses human-understandable concepts.Just because your thoughts are built using your own concepts, does not mean your concepts can describe how your thoughts are computed. Or:The existence of a natural-language description of a thought (like "I want ice cream") doesn't mean that your brain computed that thought in a way which can be compactly described by familiar concepts... (read more)ReplyWhen do utility functions constrain?19HoagyUtility FunctionsFrontpage4yThe ProblemThis post is an exploration of a very simple worry about the concept of utility maximisers - that they seem capable of explaining any exhibited behaviour.  It is one that has, in different ways, has been brought up many times before. Rohin Shah, for example, complained that the behaviour of everything from robots to rocks can be described by utility functions. The conclusion seems to be that being an expected utility maximiser tells us nothing at all about the way a decision maker acts in the world - the utility function does not constrain. This clashes with arguments that suggest, for example, that a future humanity or AI would wish to self-modify its preferences to be representable as a utility function.According to Wikipedia’s definition, a...(Continue Reading  – 1992 more words)Chris_Leong3d1What we need to find, for a given agent to be constrained by being a 'utility maximiser' is to consider it as having a member of a class of utility functions where the actions that are available to it systematically alter the expected utility available to it - for all utility functions within this class.This sentence is extremely difficult for me to parse. Any chance you could clarify it?Reply1Chris_Leong3dCould you explain smoothness is typically required for meaningly constraining
our actions?Explaining grokking through circuit efficiency48Vikrant Varma, Rohin ShahInterpretability (ML & AI)Machine Learning  (ML)World ModelingAIFrontpage6dThis is a linkpost for https://arxiv.org/abs/2309.02390This is a linkpost for our paper Explaining grokking through circuit efficiency, which provides a general theory explaining when and why grokking (aka delayed generalisation) occurs, and makes several interesting and novel predictions which we experimentally confirm (introduction copied below). You might also enjoy our explainer on X/Twitter.AbstractOne of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation. We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do...(See More – 595 more words)3Algon5dWhich of these theories:

can predict the same "four novel predictions about grokking" yours did? The
relative likelihoods are what matters for updates after all.

Also, how does this theory explain other grokking related pheonmena e.g.
Omni-Grok? And how do things change as you increase parameter count? Scale
matters, and I am not sure whether things like ungrokking would vanish with
scale as catastrophic forgetting did. Or those various inverse scaling
phenomena.Rohin Shah4d44Which of these theories [...] can predict the same "four novel predictions about grokking" yours did? The relative likelihoods are what matters for updates after all.I disagree with the implicit view on how science works. When you are a computationally bounded reasoner, you work with partial hypotheses, i.e. hypotheses that only make predictions on a small subset of possible questions, and just shrug at other questions. This is mostly what happens with the other theories:Difficulty of representation learning: Shrugs at our prediction about Cmem /... (read more)Reply4Algon5dIs this the same thing as catastrophic forgetting?4Rohin Shah4dFrom page 6 of the paper:

(All of these predictions are then confirmed in the experimental section.)Load More





