Home | Cohere For AIProgramsWho we areSpotlight PapersFAQVideosExploring the unknown, togetherCohere For AI is a non-profit research lab that seeks to solve complex machine learning problems. We support fundamental research that explores the unknown, and are focused on creating more points of entry into machine learning research.Curiosity-driven collaborationWe are committed to making meaningful progress in machine learning research through open collaboration. We believe that technology is powerful, and empowering different perspectives ensures responsible innovation.Fundamental research labWe contribute to progress in machine learning through fundamental research. We see contributions to traditional conferences and publications in journals as an important part of our work, but also support efforts that go “beyond the research paper” and encourage scientific communication through different mediums.Meet our StaffScholars ProgramThe Scholars Program provides the opportunity to work alongside some of the best researchers and engineering expertise in the world — exploring the unknown, together. It will serve as an open, supportive environment that provides an alternative point of entry into NLP research.Accepted applicants, will join a dedicated team of passionate researchers and industry experts from January 2024 to August 2024, and will be paired with a project proposal, allowing you to grow as a researcher. Participation is full-time and paid. As part of the program, Scholars will have access to a large-scale experimental framework, world class research experts and will help advance our commitment to supporting responsible, fundamental research on machine learning topics while prioritizing good stewardship of open source scientific practices.Learn moreAya: An Open Science InitiativeAya is a global project that aims to build a multilingual language model via instruction tuning that harnesses the collective wisdom and contributions of people from all over the world. The goal is to make language model development more accessible and collaborative and to address the under-representation of certain languages in natural language processing research. Aya is open to anyone who is passionate about advancing the field of natural language processing and is committed to promoting open science. Learn more about the project in this blog post.Join the Aya Discord server and start contributing in your language today.Start contributing to AyaOur Open Science CommunityWe’re not just another research group. We are the open science community to conduct top-tier ML research while creating more points of entry into the field. Our research community is a space where researchers, engineers, linguists, social scientists, and lifelong learners connect and collaborate with each other. We come together from over 100 countries around the world and support large and small scale research collaborations. Join UsResearch Grant ProgramCohere For AI research grants are designed to support academic partners who are conducting research with the goal of releasing a peer-reviewed scientific artifact. Our program provides academic partners, developers, researchers, and other members of our community with subsidized access to the Cohere API. We are interested in supporting requests for API access that enable data for good applications of large language models (LLMs), and/or responsible use of LLMs. Learn more about the goals of this program on our blog.Apply now EventsWe bring together leading researchers and rising stars in the field of machine learning to discuss their research learning journeys and showcase their technical achievements. Research is inherently a human endeavor, and our event series provide insights from beginning to breakthrough. To stay up to date on upcoming talks, sign up to our mailing listSign up to our mailing listSpotlight papersIntriguing Properties of Quantization at ScaleAre emergent quantization difficulties of LLM truly inherent to scale, or can they be altered and conditioned by optimization choices made during pre-training?Authors: Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Stephen Gou, Phil Blunsom, Ahmet Üstün, Sara HookerRead the paperOn the Challenges of Using Black-Box APIs for Toxicity Evaluation in ResearchHow changes to the Perspective API, used widely for toxicity evaluation, impact research reproducibility and rankings of model risk.Authors: Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara HookerRead the paperMetadata Archaeology: Unearthing Data Subsets by Leveraging Training DynamicsProviding a unified and efficient framework for Metadata Archaeology – uncovering and inferring metadata of examples in a dataset.Authors: Shoaib Ahmed Siddiqui, Nitarshan Rajkumar, Tegan Maharaj, David Krueger, Sara HookerRead the paperEfficient Methods for Natural Language Processing: A SurveySynthesizing methods and findings in NLP efficiencies, guiding new researchers in the field, and inspiring the development of new methods.Authors: Marcos Treviso, Tianchu Ji, Ji-Ung Lee, Betty van Aken, Qingqing Cao, Manuel R. Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Pedro H. Martins, André F. T. Martins, Peter Milder, Colin Raffel, Edwin Simpson, Noam Slonim, Niranjan Balasubramanian, Leon Derczynski, Roy SchwartzRead the paperIntriguing Properties of Compression on Multilingual ModelsExploring compression as a way to improve model robustness for low-resource languages.Authors: Kelechi Ogueji, Orevaoghene Ahia, Gbemileke Onilude, Sebastian Gehrmann, Sara Hooker, Julia KreutzerRead the paperLarge Language Models are not Zero Shot CommunicatorsInvestigating the implicature gap in Large Language Models.Authors: Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktäschel, Edward GrefenstetteRead the paperMore PapersWork by Cohere For AI and Technical Staff at CohereAdversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image ModelsAuthors: Alicia Parrish, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, Rafael Mosquera, Addison Howard, Will Cukierski, D. Sculley, Vijay Janapa Reddi, Lora Aroyo.Read the paperThe Presidio Recommendations on Responsible Generative AI - World Economic ForumAuthors: Sara Hooker, and over 100 other thought leaders.Read the recommendationsLanguage Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought PromptingAuthors: Miles Turpin, Julian Michael, Ethan Perez, Samuel R. BowmanRead the paperFAIR-Ensemble: When Fairness Naturally Emerges From Deep EnsemblingAuthors: Wei-Yin Ko, Daniel D’souza, Karina Nguyen, Randall Balestriero, Sara HookerRead the paperAssociative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception Authors: Uday Kamal, Saurabh Dash, Saibal MukhopahdyayRead the paperPASHA: Efficient HPO and NAS with Progressive Resource AllocationAuthors: Authors: Andrej Bohdal, Lukas Balles, Martin Wistuba, Beyza Ermis, Cedric Archambeau, Giovanni ZappellaRead the paperBigScience: A Case Study in the Social Construction of a Multilingual Large Language ModelAuthors: Christopher Akiki, Giada Pistilli, Margot Mieskes, Matthias Gallé, Thomas Wolf, Suzana Ilic, Yacine JerniteRead the paperMTEB: Massive Text Embedding BenchmarkAuthors: Niklas Muennighoff, Nouamane Tazi, Loïc Magne, Nils ReimersRead the paperImproving Policy Learning via Language Dynamics DistillationAuthors: Victor Zhong, Jesse Mu, Luke Zettlemoyer, Edward Grefenstette, Tim RocktäschelRead the paperPrioritized Training on Points that are Learnable, Worth Learning, and Not Yet LearntAuthors: Sören Mindermann, Jan Brauner, Muhammed Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt Höltgen, Aidan N. Gomez, Adrien Morisot, Sebastian Farquhar, Yarin Galarxiv.orgStudying the Impact of Magnitude Pruning on Contrastive Learning MethodsAuthors: Francesco Corti, Rahim Entezari, Sara Hooker, Davide Bacciu, Olga SaukRead the paperRobust Distillation for Worst-class Performance Authors: Serena Wang, Harikrishna Narasimhan, Yichen Zhou, Sara Hooker, Michal Lukasik, Aditya Krishna MenonRead the paperLifting the Veil on Hyper-parameters for Value-based Deep Reinforcement LearningAuthors: João G.M. Araújo, Johan S. Obando-Ceron, Pablo Samuel CastroRead the paperαNAS: Neural Architecture Search using Property Guided SynthesisAuthors: Charles Jin, Phitchaya Mangpo Phothilimthana, Sudip RoyRead the paperScalable Training of Language Models using PAX pjit and TPUv4Authors: Joanna Yoo, Kuba Perlin, Siddhartha Rao Kamalakara, João G.M. AraújoRead the paperMitigating Harm in Language Models with Conditional-Likelihood FiltrationAuthors: Helen Ngo, Cooper Raterink, João G.M. Araújo, Ivan Zhang, Carol Chen, Adrien Morisot, Nicholas FrosstRead the paperNo News is Good News: A Critique of the One Billion Word BenchmarkAuthors: Helen Ngo, João G.M. Araújo, Jeffrey Hui, Nicholas FrosstRead the paperExploring Low Rank Training of Deep Neural NetworksAuthors: Siddhartha Rao Kamalakara, Acyr Locatelli, Bharat Venkitesh, Jimmy Ba, Yarin Gal, Aidan N. GomezRead the paperPredicting Twitter Engagement With Deep Language ModelsAuthors: Maksim N Volkovs, Zhaoyue Cheng, Mathieu Ravaut, Hojin Yang, Kevin Shen, Jinpeng Zhou, Anson Wong, Saba Zuberi, Ivan Zhang, Nick Frosst, Helen Ngo, Carol Chen, Bharat Venkitesh, Stephen Gou, Aidan N. GomezRead the paperInterlocking Backpropagation: Improving depthwise model-parallelismAuthors: Aidan N. Gomez, Oscar Key, Kuba Perlin, Stephen Gou, Nick Frosst, Jeff Dean, Yarin Galjmlr.orgSparking great conversations, collaborations, and community VideosFireside Chat: Colin Raffel8-bit Methods for Efficient Deep Learning with Tim DettmersCommunity Talks: Mechanistic Interpretability - Getting Started with Catherine OlssonFireside Chat: Pablo Samuel CastroCommunity Talks: Rosanne Liu on Career CreationFireside Chat: Samy BengioAll VideosWho we areCohere For AI is a registered non-profit, and core to our mission statement is contributing to knowledge in the public domain. We collaborate with researchers from private and public institutions, as well as independent researchers unaffiliated with an institution.We are committed to open sourcing code from our programs, and promoting good stewardship of open source scientific practices.Our TeamHistory of For AIIn 2017, a team of friends, classmates, and engineers started a distributed research collaboration, with a focus on creating a medium for early-career AI enthusiasts to engage with experienced researchers – they called it “for.ai.” Two of those co-founding members, Aidan Gomez and Ivan Zhang, later went on to co-found Cohere, and many of the original members went on to do exciting things (pursuing PhDs, working at industry and academic labs).At the time, For AI was one of the first community-driven research groups to support independent researchers around the world. Today, Cohere is proud to reintroduce For AI as Cohere For AI, a dedicated research lab and community for exploring the unknown, together.Learn moreFrequently Asked QuestionsDo we charge for our educational programs or community membership?Cohere For AI is a registered non-profit. We do not charge for participating in any of our programs, and are committed to supporting educational outreach programs, which include compute resources and infrastructure needed to participate in machine learning research.Are you hiring for research positions or interns?Our full list of positions are listed here.Stay updatedLoading...Get InvolvedResearch LabCommunitySeminarsAboutWho we areFAQContactTwitterLinkedInEmailResearchSpotlight PapersMore Papers