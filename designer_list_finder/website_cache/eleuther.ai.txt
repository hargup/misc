







EleutherAI
















































5000























0








      Skip to Content
    










































        About
      




                  Staff
                






        Research
      




                  Language Modeling
                





                  Interpretability
                





                  Alignment
                





                  Other Modalities
                






        Papers
      



        Releases
      


Blog






































Open Menu
Close Menu











































        About
      




                  Staff
                






        Research
      




                  Language Modeling
                





                  Interpretability
                





                  Alignment
                





                  Other Modalities
                






        Papers
      



        Releases
      


Blog






































Open Menu
Close Menu























Folder:
About








Back





          Staff
        







Folder:
Research








Back





          Language Modeling
        





          Interpretability
        





          Alignment
        





          Other Modalities
        







        Papers
      





        Releases
      



Blog


















































Empowering Open-Source Artificial Intelligence Research














Explore our research






Projects

























Interpreting Across Time






In order to study what makes large language models so capable, we must investigate trends throughout training and across different model sizes. We developed our open-access Pythia models to serve as a controlled environment for such interpretability research.


























Eliciting Latent Knowledge






Language models like ChatGPT and Bing Chat often confidently say false things. As models get smarter, humans won't always be able to independently check if a model's claims are true or false. We aim to circumvent this issue by directly eliciting latent knowledge (ELK) inside the modelâ€™s activations.


























Datasets






A crucial component of LLM and NLP is the datasets used to train models. Collecting high-quality data is important for creating performant LLMs. Recent work has shown that scaling up data is just as crucial as scaling up model parameter count for efficient use of compute to train LLMs.


























Training LLMs






EleutherAI has trained and released several LLMs, and the codebases used to train them. Several of these LLMs were the largest or most capable available at the time and have been widely used since in open-source research applications.


























Evaluating LLMs






Quantifying the performance of large language models is crucial to evaluating new techniques and validating new approaches so that different model releases can be compared objectively. LLMs are generally evaluated on several benchmark datasets and given scores, which serve numeric quantities to compare across models.


























Alignment MineTest






Alignment-MineTest is a research project that uses the open source Minetest voxel engine as a platform for studying AI alignment. Alignment in this context refers to the process of ensuring that an artificial intelligence system behaves in a manner that is consistent with human values and goals.


























Text-to-Image Synthesis
































Mesaoptimization
































Improved T5






The T5 language model has unambiguously been a major advance for work with large language models. Encoder-decoder models have shown substantial improvements over decoder-only models in contexts, including a zero-shot performance with MTF and image synthesis and comparatively less memorization. 


























Polyglot






The Polyglot Project focuses on extending the benefits of large language models to cultures and contexts not well suited by the current state of affairs, as well as studying the best practices for doing so. This work includes training LLMs in languages other than English and Chinese, improving tools for non-English data documentation, curation, and analysis, culturally-aware research on ethics and bias in non-English LLMs, and more.


























Aesthetic Models



















Recent Publications






Featured












31 Aug 2023



arXiv



YaRN: Efficient Context Window Extension of Large Language Models



31 Aug 2023



arXiv





31 Aug 2023



arXiv









30 Jun 2023



arXiv



Stay on topic with Classifier-Free Guidance



30 Jun 2023



arXiv





30 Jun 2023



arXiv









7 Jun 2023



arXiv



A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models



7 Jun 2023



arXiv





7 Jun 2023



arXiv









6 Jun 2023



arXiv



LEACE: Perfect linear concept erasure in closed form



6 Jun 2023



arXiv





6 Jun 2023



arXiv









3 Jun 2023



arXiv



GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration



3 Jun 2023



arXiv





3 Jun 2023



arXiv


























AboutResearch Language Modeling Interpretability Alignment Other ModalitiesReleasesBlog



contact@eleuther.ai






































Copyright EleutherAI 2023















